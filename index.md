---
layout: default
---

# Hi, I'm Alex

I'm interested in machine learning and mechanistic interpretability — understanding *what* models learn and *why* they learn it. I like building small, fully interpretable systems where every parameter can be examined, and using controlled experiments to separate real understanding from pattern matching.

Currently exploring how transformer models represent structured game knowledge, and whether tiny models develop compositional reasoning or rely on statistical shortcuts.

[GitHub](https://github.com/alexayvazyan)

---

## Writing

- [Mechanistic Interpretability of a 2,424-Parameter TFT Transformer](/2026/02/19/tft-mechanistic-interpretability.html) — Dissecting what a tiny placement-prediction model actually learns about Teamfight Tactics.
